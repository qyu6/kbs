{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "9054da90",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<T.Knowledge.Database>📂:回归\n",
      "<class 'str'>\n",
      "{'回归'}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <table>\n",
       "        <thead>\n",
       "            <th> Keywords </th><th> Content </th>\n",
       "        </thead>\n",
       "    <tbody><tr><td> LASSO<span style='color:red;background-color:yellow'> 回归 </span> (Least Absolute Shrinkage and Selection Operator)<span style='color:red;background-color:yellow'> 回归 </span> </td><td> 该方法是一种压缩估计。它通过构造一个惩罚函数得到一个较为精炼的模型，使得它压缩一些<span style='color:red;background-color:yellow'> 回归 </span>系数，即强制系数绝对值之和小于某个固定值；同时设定一些<span style='color:red;background-color:yellow'> 回归 </span>系数为零。因此保留了子集收缩的优点，是一种处理具有复共线性数据的有偏估计。Lasso<span style='color:red;background-color:yellow'> 回归 </span>有时也叫做线性<span style='color:red;background-color:yellow'> 回归 </span>的L1正则化，和Ridge<span style='color:red;background-color:yellow'> 回归 </span>的主要区别就是在正则化项，Ridge<span style='color:red;background-color:yellow'> 回归 </span>用的是L2正则化，而Lasso<span style='color:red;background-color:yellow'> 回归 </span>用的是L1正则化。 </td></tr><tr><td> Ridge<span style='color:red;background-color:yellow'> 回归 </span> 岭<span style='color:red;background-color:yellow'> 回归 </span> </td><td> 在机器学习中，如果特征很多，但是训练数据量不够大的情况下，学习器很容易把特有的一些特点也当做是整个样本空间的一般性质进行学习，这就会出现过拟合的现象，线性<span style='color:red;background-color:yellow'> 回归 </span>模型也不例外。对于过拟合，在模型层面上我们一般会在模型中加入正则化项来优化模型，正则化项一般分为两种：L1正则和L2正则。线性<span style='color:red;background-color:yellow'> 回归 </span>的L2正则称为Ridge<span style='color:red;background-color:yellow'> 回归 </span>（岭<span style='color:red;background-color:yellow'> 回归 </span>），其与普通线性<span style='color:red;background-color:yellow'> 回归 </span>模型的区别是在损失函数上增加了一个L2正则项。Ridge<span style='color:red;background-color:yellow'> 回归 </span>（L2正则）会倾向于让模型选择更小的参数，但不会使参数为0，所以L2正则不会减少模型的特征；而Lasso<span style='color:red;background-color:yellow'> 回归 </span>（L1正则）能使一些“不重要”的特征的参数为0，相当于丢弃了这个特征，简化了模型。 </td></tr><tr><td> AR(Auto-Regression), 自<span style='color:red;background-color:yellow'> 回归 </span> </td><td> 自<span style='color:red;background-color:yellow'> 回归 </span>。是统计学上一种处理时间序列的方法，利用过往x各期的变现来预测x本期的表现。不是用x来预测y，而是用x来预测x，所以叫自<span style='color:red;background-color:yellow'> 回归 </span>。 </td></tr><tr><td> YOLO(You Only Look Once) </td><td> YOLO把目标检测问题简化成了一个<span style='color:red;background-color:yellow'> 回归 </span>问题，直接从图像像素出发，去得到框和分类概率。这也是为了特别突出YOLO区别于两阶段算法的特点，从名字就可以感受到，YOLO算法速度很快，事实上也是如此。YOLO用整个图片的特征去预测每一个边界框。它还同时预测一个图像在所有类中的所有边界框。YOLO先把整个图片划分成S*S个方格，如果一个物体的中心正好落在一个方格中，那么这个方格就负责来预测物体。每一个方格预测出B个边界框和这些框的置信分数 </td></tr><tr><td> CART(Classification and Regression Tree):分类<span style='color:red;background-color:yellow'> 回归 </span>树 </td><td> CART生成的过程其实就是特征选择的过程，首先遍历每个特征，然后遍历每个特征所有可能的切分点，找到最优特征j对应的最优特征m。<span style='color:red;background-color:yellow'> 回归 </span>树可以选择平方误差、分类数可以选择基尼系数 </td></tr><tr><td> ARMA(Auto-Regression Integrated Moving Average),自<span style='color:red;background-color:yellow'> 回归 </span>移动平均混合模型 </td><td> 自<span style='color:red;background-color:yellow'> 回归 </span>移动平均混合模型。是研究时间序列的重要方法，由自<span style='color:red;background-color:yellow'> 回归 </span>（AR）和移动平均（MA）混合组成。模型参量法高分辨率谱分析方法之一。这种方法是研究平稳随机过程有理谱的典型方法，适用于很大一类实际问题。它比AR模型法与MA模型法有较精确的谱估计及较优良的谱分辨率性能，但其参数估算比较繁琐 </td></tr><tr><td> ARIMA(Auto-Regression Integrated Moving Average)，差分整合移动自<span style='color:red;background-color:yellow'> 回归 </span>模型 </td><td> 差分整合移动自<span style='color:red;background-color:yellow'> 回归 </span>模型，或叫“整合移动平均自<span style='color:red;background-color:yellow'> 回归 </span>模型”。ARIMA(p，d，q)中，AR是“自<span style='color:red;background-color:yellow'> 回归 </span>”，p为自<span style='color:red;background-color:yellow'> 回归 </span>项数；MA为“滑动平均”，q为滑动平均项数；d为使之成为平稳序列所做的差分次数（阶数） </td></tr><tr><td> Discriminative Model: 判别模型 </td><td> 监督学习的一种，根据特征X预测标记Y，求得概率P(Y|X)。对于新的特征X输入，可以直接得出标记Y，相当于直接得到了明确的判别边界。比如线性<span style='color:red;background-color:yellow'> 回归 </span>模型，支持向量机等 </td></tr><tr><td> CRF(Conditional Random Fields):条件随机场 </td><td> 是给定一组输入序列条件下另一组输出序列的条件概率分布模型，在自然语言处理中得到了广泛应用，常用于词性标注。条件随机场是逻辑<span style='color:red;background-color:yellow'> 回归 </span>的序列化版本。逻辑<span style='color:red;background-color:yellow'> 回归 </span>是用于分类的对数线性模型，条件随机场是用于序列化标注的对数线性模型。对于词性标注问题，HMM模型也可以解决。HMM的思路是用生成办法，就是说，在已知要标注的句子s的情况下，去判断生成标注序列l的概率。每一个HMM模型都等价于某个CRF，但CRF比HMM更有优势的两点是：1.可以定义数量更多，种类更丰富的特征函数（HMM是生成模型，只能由前序标签来生成）；2.可以使用任意的权重（HMM特征函数为log形式概率，只能小于0） </td></tr><tr><td> Softmax：对向量进行Softmax归一化 </td><td> 经过Softmax之后，连续数值转化为相对概率（和为1，即被理解为归一化的过程），输出表征了不同类别之间的相对概率。Softmax regression，又被称作多项逻辑<span style='color:red;background-color:yellow'> 回归 </span>(multinomial logistic regression)，是逻辑<span style='color:red;background-color:yellow'> 回归 </span>在处理多类别任务上的推广 </td></tr><tr><td> R-CNN(Region-Convolutional Neural Network), 区域卷积神经网络 </td><td> R-CNN的全称是Region-CNN，是第一个成功将深度学习应用到目标检测上的算法。R-CNN基于卷积神经网络(CNN)，线性<span style='color:red;background-color:yellow'> 回归 </span>，和支持向量机(SVM)等算法，实现目标检测技术。传统的目标检测方法大多以图像识别为基础。一般可以在图片上使用穷举法选出所所有物体可能出现的区域框，对这些区域框提取特征并使用图像识别方法分类， 得到所有分类成功的区域后,通过非极大值抑制(Non-Maximum Suppression)输出结果。R-CNN遵循传统目标检测的思路，同样采用提取框，对每个框提取特征、图像分类、 非极大值抑制四个步骤进行目标检测。只不过在提取特征这一步，将传统的特征(如 SIFT、HOG 特征等)换成了深度卷积网络提取的特征 </td></tr><tr><td> 独热编码（One-Hot）,一种数据预处理的技术手段 </td><td> one-hot编码是将类别变量转换为机器学习算法中容易处理的一种形式。在使用one-hot编码中，我们可以将离散特征的取值扩展到欧式空间，在机器学习中，我们的研究范围就是在欧式空间中，首先这一步，保证了能够适用于机器学习中；而；另外了对于one-hot处理的离散的特征的某个取值也就对应了欧式空间的某个点！在统计机器学习算法中的<span style='color:red;background-color:yellow'> 回归 </span>，分类这些问题中，特征之间距离的计算或相似度计算非常重要，比如大家常用的k-means，而我们常用的这些计算都在欧式空间中进行相似度计算。换句话说，就是我上面说的研究范围在欧式空间，保证了one-hot编码的成立！ </td></tr><tr><td> 随机森林(Random Forest)和梯度提升树(GBDT)的差异 </td><td> (1)随机森林采用的bagging思想，而GBDT采用的boosting思想。这两种方法都是Bootstrap思想的应用，Bootstrap是一种有放回的抽样方法思想。虽然都是有放回的抽样，但二者的区别在于：Bagging采用有放回的均匀取样，而Boosting根据错误率来取样（Boosting初始化时对每一个训练样例赋相等的权重1／n，然后用该算法对训练集训练t轮，每次训练后，对训练失败的样例赋以较大的权重），因此Boosting的分类精度要优于Bagging。Bagging的训练集的选择是随机的，各训练集之间相互独立，弱分类器可并行，而Boosting的训练集的选择与前一轮的学习结果有关，是串行的。(2)组成随机森林的树可以是分类树，也可以是<span style='color:red;background-color:yellow'> 回归 </span>树；而GBDT只能由<span style='color:red;background-color:yellow'> 回归 </span>树组成。(3)组成随机森林的树可以并行生成；而GBDT只能是串行生成。(4)对于最终的输出结果而言，随机森林采用多数投票等；而GBDT则是将所有结果累加起来，或者加权累加起来。(5)随机森林对异常值不敏感；GBDT对异常值非常敏感。(6)随机森林对训练集一视同仁；GBDT是基于权值的弱分类器的集成。(7)随机森林是通过减少模型方差提高性能；GBDT是通过减少模型偏差提高性能 </td></tr><tr><td> XGBoost (eXtreme Gradient Boosting), 极致梯度提升。和GBDT的主要差异？ </td><td> 是一种基于GBDT的算法或者说工程实现。XGBoost的基本思想和GBDT相同，但是做了一些优化，比如二阶导数使损失函数更精准；正则项避免树过拟合；Block存储可以并行计算等。[XGBoost比GBDT好的地方]：二阶泰勒展开；节点分数惩罚正则；增益计算不同，gbdt是gini，xgb是优化推导公式。[XGBoost原始Paper内容：XGBoost是GB算法的高效实现，xgboost中的基学习器除了可以是CART（gbtree）也可以是线性分类器（gblinear）。(1). xgboost在目标函数中显示的加上了正则化项，基学习为CART时，正则化项与树的叶子节点的数量T和叶子节点的值有关；(2). GB中使用Loss Function对f(x)的一阶导数计算出伪残差用于学习生成fm(x)，xgboost不仅使用到了一阶导数，还使用二阶导数。做二阶泰勒展开：g为一阶导数，h为二阶导数；(3). 上面提到CART<span style='color:red;background-color:yellow'> 回归 </span>树中寻找最佳分割点的衡量标准是最小化均方差，xgboost寻找分割点的标准是最大化，lamda，gama与正则化项相关] xgboost与gdbt除了上述三点的不同，xgboost在实现时还做了许多优化：（1）在寻找最佳分割点时，考虑传统的枚举每个特征的所有可能分割点的贪心法效率太低，xgboost实现了一种近似的算法。大致的思想是根据百分位法列举几个可能成为分割点的候选者，然后从候选者中根据上面求分割点的公式计算找出最佳的分割点。（2）xgboost考虑了训练数据为稀疏值的情况，可以为缺失值或者指定的值指定分支的默认方向，这能大大提升算法的效率，paper提到50倍。（3）特征列排序后以块的形式存储在内存中，在迭代中可以重复使用；虽然boosting算法迭代必须串行，但是在处理每个特征列时可以做到并行。（4）按照特征列方式存储能优化寻找最佳的分割点，但是当以行计算梯度数据时会导致内存的不连续访问，严重时会导致cache miss，降低算法效率。paper中提到，可先将数据收集到线程内部的buffer，然后再计算，提高算法的效率。（5）xgboost 还考虑了当数据量比较大，内存不够时怎么有效的使用磁盘，主要是结合多线程、数据压缩、分片的方法，尽可能的提高算法的效率。 </td></tr></tbody></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from os.path import exists\n",
    "import openpyxl\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "pd.set_option('display.max_colwidth',None)\n",
    "\n",
    "result = 'searchoutput.csv'\n",
    "if exists(result):\n",
    "    os.remove(result)\n",
    "    \n",
    "# 创建结果文件\n",
    "wbResult = openpyxl.Workbook()\n",
    "wsResult = wbResult.worksheets[0]\n",
    "wsResult.append(['result'])\n",
    "\n",
    "# 读取原表两次，一次用来进行建表输入，一次用来做对应的输入\n",
    "wb = openpyxl.load_workbook('SourceDB.xlsx')\n",
    "input_excel = 'SourceDB.xlsx'\n",
    "data = pd.read_excel(input_excel)\n",
    "ws = wb.worksheets[0]\n",
    "\n",
    "# 原表空白部分用*填充\n",
    "for k in range(1,ws.max_column+1):\n",
    "    for i in range(1,ws.max_row+1):\n",
    "        if ws.cell(row=i,column=k).value is None:\n",
    "            ws.cell(i,k,'****')\n",
    "\n",
    "\n",
    "input_word = input(\"<T.Knowledge.Database>📂:\").strip().lower()\n",
    "# st.subheader('🐼[T.Q Knowledge Base]')\n",
    "# input_word1 = st.text_input('©TAILab|Last release:2022/3/3','')\n",
    "# input_word = input_word1.strip().lower()\n",
    "print(type(input_word))\n",
    "input_word_exist = re.sub(u\"([u4e00-\\u9fa5\\u0030-\\u0039\\u0041-\\u005a\\u0061-\\u007a])\",\"\",input_word)\n",
    "input_word = input_word.split()\n",
    "\n",
    "\n",
    "\n",
    "result_list = []\n",
    "for index,row in enumerate(ws.rows):\n",
    "\n",
    "    if index == 0:\n",
    "        continue\n",
    "    rs_list = list(map(lambda cell: cell.value, row))\n",
    "    list_str = \"\".join('%s' %id for id in rs_list).replace(\"\\n\",\" \").replace(\"\\n\",\" \").replace(\"\\t\",\" \").replace(\"\\r\",\" \").lower()\n",
    "    result_list.append([index, list_str])\n",
    "\n",
    "\n",
    "\n",
    "def search_onebyone(input_word_exist, input_word_list, result_list):\n",
    "    new_list = []\n",
    "    dict_list = []\n",
    "    new_list_count = []\n",
    "    # 精确匹配\n",
    "    for i in range(len(result_list)):\n",
    "        for m in input_word_list:\n",
    "            pattern = m\n",
    "            regex = re.compile(pattern)\n",
    "            nz = regex.search(result_list[i][1])\n",
    "            if nz:\n",
    "                new_list.append([len(nz.group()),nz.start(),result_list[i][0]-1])\n",
    "                new_list_count.append(result_list[i][0]-1)\n",
    "\n",
    "    new_list = sorted(new_list)\n",
    "    new_index = [x for _,_,x in new_list]\n",
    "    new_index = sorted(set(new_index),key=new_index.index)\n",
    "\n",
    "    # 计数，只有当输入的全部单词全部出现以后，才取出\n",
    "    dict_list.append([k for k,v in Counter(new_list_count).items() if v == len(input_word_list)])\n",
    "    for m in dict_list:\n",
    "        result_index = m\n",
    "    temp = [j for j in new_index if j in result_index]\n",
    "    return temp\n",
    "result = search_onebyone(input_word_exist, input_word, result_list)\n",
    "\n",
    "data.loc[(x for x in result)].to_csv('searchoutput.csv', encoding= 'utf_8_sig')\n",
    "\n",
    "\n",
    "# 高亮显示关键词匹配结果\n",
    "dftest = data.loc[(x for x in result)]\n",
    "\n",
    "import re\n",
    "from IPython.display import HTML\n",
    "\n",
    "def display_highlighted_words(df, keywords):\n",
    "    head = \"\"\"\n",
    "    <table>\n",
    "        <thead>\n",
    "            \"\"\" + \\\n",
    "                \"\".join([\"<th> %s </th>\" % c for c in df.columns])\\\n",
    "                + \"\"\"\n",
    "        </thead>\n",
    "    <tbody>\"\"\"\n",
    "\n",
    "#     head = \"\"\"\n",
    "#     <table>\n",
    "#         <thead>\n",
    "#             <th> Keywords </th><th> Content </th>\n",
    "#         </thead>\n",
    "#     </table>\n",
    "#     \"\"\"\n",
    "\n",
    "    for i,r in df.iterrows():\n",
    "        row = \"<tr>\"\n",
    "        for c in df.columns:\n",
    "            matches = []\n",
    "            for k in keywords:\n",
    "                for match in re.finditer(k, str(r[c])):\n",
    "                    matches.append(match)\n",
    "\n",
    "            # reverse sorting\n",
    "            matches = sorted(matches, key = lambda x: x.start(), reverse=True)\n",
    "\n",
    "            # building HTML row\n",
    "            cell = str(r[c])\n",
    "\n",
    "            # print(cell)\n",
    "            for match in matches:\n",
    "                cell = cell[:match.start()] +\\\n",
    "                    \"<span style='color:red;background-color:yellow'> %s </span>\" % cell[match.start():match.end()] +\\\n",
    "                    cell[match.end():]\n",
    "\n",
    "            row += \"<td> %s </td>\" % cell\n",
    "\n",
    "        row += \"</tr>\"\n",
    "        head += row\n",
    "        \n",
    "    head += \"</tbody></table>\"\n",
    "    \n",
    "    display(HTML(head))\n",
    "\n",
    "\n",
    "\n",
    "# 特殊高亮匹配规则：\n",
    "# 原始英文单词\n",
    "input_word\n",
    "\n",
    "# 英文全部大写\n",
    "temp = ' '.join(input_word)\n",
    "output1 = temp.upper()\n",
    "output1a = output1.split()\n",
    "\n",
    "# 英文首字母大写\n",
    "output2 = []\n",
    "for i in input_word:\n",
    "    temp = i.capitalize()\n",
    "    output2.append(temp)\n",
    "    \n",
    "# 英文首字母小写，其他大写\n",
    "temp = ' '.join(input_word)\n",
    "output3 = []\n",
    "for i in input_word:\n",
    "    temp1 = i[0]+i[1:].upper()\n",
    "    output3.append(temp1)\n",
    "\n",
    "# 英文第二个字母小写，其他大写\n",
    "temp = ' '.join(input_word).upper()\n",
    "input_word1 = temp.split()\n",
    "output4 = []\n",
    "for i in input_word1:\n",
    "    if len(i)>2:\n",
    "        temp1 = i[0]+i[1].lower()+i[2:]\n",
    "#         print(temp1)\n",
    "        output4.append(temp1)\n",
    "    \n",
    "    \n",
    "\n",
    "# 英文前三个字母大写，其余小写\n",
    "temp = ' '.join(input_word).upper()\n",
    "input_word1 = temp.split()\n",
    "output5 = []\n",
    "for i in input_word1:\n",
    "    if len(i)>4:\n",
    "        temp1 = i[0:3]+i[3:].lower()\n",
    "#         print(temp1)\n",
    "        output5.append(temp1)\n",
    "\n",
    "\n",
    "final_key = set(input_word + output1a + output2 + output3 + output4 + output5)\n",
    "print(final_key)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "# 启动知识库搜索引擎API：\n",
    "display_highlighted_words(dftest, final_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde26022",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2636b3e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
